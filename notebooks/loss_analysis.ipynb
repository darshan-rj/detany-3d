{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1bdb735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///app/DetAny3D\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: detect_anything\n",
      "  Building editable for detect_anything (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for detect_anything: filename=detect_anything-1.0-0.editable-py3-none-any.whl size=7037 sha256=6278da300aa9d6accdb92c3b4418cfeae33425d4897e4ce586fe4dcab6634e28\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-jqzrx3ul/wheels/42/47/cb/8136801cab19776f09472649003584ab5b5c5605be54d81d2c\n",
      "Successfully built detect_anything\n",
      "Installing collected packages: detect_anything\n",
      "Successfully installed detect_anything-1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install -e ../\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a98175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added /app/DetAny3D to system path.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Set this to the directory containing the 'detany3d' source folder\n",
    "# For example, if you cloned it to '/content/detany3d'\n",
    "repo_path = '/app/DetAny3D' \n",
    "if repo_path not in sys.path:\n",
    "    sys.path.append(repo_path)\n",
    "    print(f\"Added {repo_path} to system path.\")\n",
    "\n",
    "from detect_anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8a9bedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "from nuscenes.utils.data_classes import Box\n",
    "from pyquaternion import Quaternion\n",
    "from detect_anything.modeling import ImageEncoderViT, MaskDecoder, PromptEncoder, TwoWayTransformer\n",
    "\n",
    "# Ensure Matplotlib plots are displayed inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5ed347d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing NuScenes dataset (version: v1.0-trainval)...\n",
      "Initialization complete.\n"
     ]
    }
   ],
   "source": [
    "# --- USER INPUT REQUIRED ---\n",
    "# ⚠️ Update these paths to match your local setup ⚠️\n",
    "NUSCENES_ROOT = \"/data/nuscenes\" \n",
    "VERSION = \"v1.0-trainval\" \n",
    "CONFIG_PATH = \"/app/DetAny3D/detect_anything/configs/loss_analysis.yaml\"\n",
    "CHECKPOINT_PATH = \"/app/DetAny3D/checkpoints/detany3d_ckpts/zero_shot_category_ckpt-004.pth\"\n",
    "CAM_CHANNEL = \"CAM_FRONT\"\n",
    "# -----------------------------\n",
    "\n",
    "# Initialize the nuScenes object\n",
    "print(f\"Initializing NuScenes dataset (version: {VERSION})...\")\n",
    "try:\n",
    "    nusc = NuScenes(version=VERSION, dataroot=NUSCENES_ROOT, verbose=False)\n",
    "    print(\"Initialization complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing nuScenes: {e}. Please check NUSCENES_ROOT.\")\n",
    "    nusc = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5b58ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_period(val, offset=0.5, period=2 * np.pi):\n",
    "    \"\"\"Limit the angle value into [offset, offset + period].\"\"\"\n",
    "    return val - np.floor(val / period + offset) * period\n",
    "\n",
    "def get_box_corners(box_7dim):\n",
    "    \"\"\"Converts a 7-DoF box [x, y, z, l, w, h, yaw] into 8x3 corner points (camera frame).\"\"\"\n",
    "    center = box_7dim[0:3]\n",
    "    dims = box_7dim[3:6]\n",
    "    yaw = box_7dim[6]\n",
    "    \n",
    "    l, w, h = dims\n",
    "    unrotated_corners = np.array([\n",
    "        [-l/2, -w/2, -h/2], [ l/2, -w/2, -h/2], [ l/2,  w/2, -h/2], [-l/2,  w/2, -h/2],\n",
    "        [-l/2, -w/2,  h/2], [ l/2, -w/2,  h/2], [ l/2,  w/2,  h/2], [-l/2,  w/2,  h/2]\n",
    "    ]).T\n",
    "\n",
    "    # Rotation Matrix (Yaw around Y-axis, simplified camera frame)\n",
    "    cos_t, sin_t = np.cos(yaw), np.sin(yaw)\n",
    "    R = np.array([\n",
    "        [cos_t, 0, sin_t],\n",
    "        [0, 1, 0],\n",
    "        [-sin_t, 0, cos_t]\n",
    "    ])\n",
    "    \n",
    "    corners_3d = (R @ unrotated_corners) + center[:, np.newaxis]\n",
    "    return corners_3d.T # Returns 8x3 array\n",
    "\n",
    "def project_3d_to_2d(corners_3d, calib_matrix, color, linestyle, label=None, return_coords=False):\n",
    "    \"\"\"Projects 3D points to 2D image coordinates using the 3x4 P-matrix.\"\"\"\n",
    "    P = calib_matrix\n",
    "    \n",
    "    # 1. Project points (converting to homogeneous coordinates)\n",
    "    points_h = np.hstack((corners_3d, np.ones((corners_3d.shape[0], 1))))\n",
    "    points_2d_h = P @ points_h.T \n",
    "    \n",
    "    # 2. Normalize and convert to image coordinates\n",
    "    points_2d = (points_2d_h[:2] / points_2d_h[2]).T\n",
    "    \n",
    "    if return_coords:\n",
    "        # Return coordinates for scatter plots\n",
    "        return points_2d[:, 0], points_2d[:, 1]\n",
    "    \n",
    "    # 3. Drawing logic for bounding box edges\n",
    "    edges = [\n",
    "        (0, 1), (1, 2), (2, 3), (3, 0),  \n",
    "        (4, 5), (5, 6), (6, 7), (7, 4),  \n",
    "        (0, 4), (1, 5), (2, 6), (3, 7)   \n",
    "    ]\n",
    "\n",
    "    ax = plt.gca()\n",
    "    for i, j in edges:\n",
    "        x_coords = [points_2d[i, 0], points_2d[j, 0]]\n",
    "        y_coords = [points_2d[i, 1], points_2d[j, 1]]\n",
    "        ax.plot(x_coords, y_coords, color=color, linestyle=linestyle, linewidth=2, zorder=5)\n",
    "\n",
    "    if label:\n",
    "        ax.plot([], [], color=color, linestyle=linestyle, linewidth=2, label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c6f9b06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration loaded successfully and converted to dot-accessible object.\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from box import Box\n",
    "\n",
    "try:\n",
    "    # 1a. Load the configuration file content (Assumes it's a YAML or config-like structure)\n",
    "    # If the config is a .py file, you'd need mmcv.Config.fromfile(CONFIG_PATH) here\n",
    "    # Since previous attempts failed, we stick to the dictionary/box method:\n",
    "    with open(CONFIG_PATH, 'r', encoding='utf-8') as f:\n",
    "        cfg_dict = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "    \n",
    "    # 1b. CRITICAL FIX: Convert the dictionary to a Box object for dot notation access\n",
    "    cfg = Box(cfg_dict)\n",
    "    print(\"✅ Configuration loaded successfully and converted to dot-accessible object.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR in Config Loading: {e}. Using NoneType placeholder.\")\n",
    "    cfg = None \n",
    "\n",
    "# --- 2. MODEL ARCHITECTURE BUILDER (FIXED) ---\n",
    "\n",
    "def build_detany3d_model(config_obj, checkpoint_path, device):\n",
    "    \"\"\" Instantiates the DetAny3D model architecture and loads weights. \"\"\"\n",
    "    \n",
    "    if config_obj is None:\n",
    "        return None\n",
    "\n",
    "    # CRITICAL: Use the component imports that work in your environment\n",
    "    try:\n",
    "        from detect_anything.modeling import ImageEncoderViT, MaskDecoder, PromptEncoder, DetAny3D \n",
    "    except ImportError as e:\n",
    "        print(f\"❌ Error importing model components: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # 1. Instantiate the core components (Using hardcoded or default params)\n",
    "        cfg_model = config_obj.model # Assume model parameters are under a 'model' key\n",
    "\n",
    "        image_encoder = ImageEncoderViT(\n",
    "            depth=12, embed_dim=768, img_size=1024, mlp_ratio=4,\n",
    "            norm_layer=torch.nn.LayerNorm, num_heads=12, qkv_bias=True, use_rel_pos=True,\n",
    "            window_size=14, global_attn_indexes=[2, 5, 8, 11],\n",
    "            cfg=config_obj # Pass the entire config object\n",
    "        )\n",
    "        prompt_encoder = PromptEncoder(\n",
    "            embed_dim=256, image_embedding_size=(64, 64), input_image_size=(1024, 1024)\n",
    "        )\n",
    "        mask_decoder = MaskDecoder(\n",
    "            num_multimask_outputs=3, transformer_dim=256\n",
    "        )\n",
    "        \n",
    "        # 2. Combine components into the main DetAny3D structure\n",
    "        model = DetAny3D(\n",
    "            image_encoder=image_encoder,\n",
    "            prompt_encoder=prompt_encoder,\n",
    "            mask_decoder=mask_decoder,\n",
    "            # Placeholder for other required parameters\n",
    "        )\n",
    "        print(\"✅ Model architecture instantiated.\")\n",
    "\n",
    "        # 3. Load checkpoint weights\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        state_dict = checkpoint.get('state_dict', checkpoint)\n",
    "            \n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        print(\"✅ Model weights loaded and set to evaluation mode.\")\n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ FATAL MODEL LOAD ERROR: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6d8a383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. MODEL ARCHITECTURE BUILDER (FIXED) ---\n",
    "\n",
    "def build_detany3d_model(config_obj, checkpoint_path, device):\n",
    "    \"\"\" Instantiates the DetAny3D model architecture and loads weights. \"\"\"\n",
    "    \n",
    "    if config_obj is None:\n",
    "        return None\n",
    "\n",
    "    # CRITICAL: Use the component imports that work in your environment\n",
    "    try:\n",
    "        from detect_anything.modeling import ImageEncoderViT, MaskDecoder, PromptEncoder, TwoWayTransformer\n",
    "    except ImportError as e:\n",
    "        print(f\"❌ Error importing model components: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # 1. Instantiate the core components (Using hardcoded or default params)\n",
    "        cfg_model = config_obj.model # Assume model parameters are under a 'model' key\n",
    "\n",
    "        image_encoder = ImageEncoderViT(\n",
    "            depth=12, embed_dim=768, img_size=1024, mlp_ratio=4,\n",
    "            norm_layer=torch.nn.LayerNorm, num_heads=12, qkv_bias=True, use_rel_pos=True,\n",
    "            window_size=14, global_attn_indexes=[2, 5, 8, 11],\n",
    "            cfg=config_obj # Pass the entire config object\n",
    "        )\n",
    "        prompt_encoder = PromptEncoder(\n",
    "            embed_dim=256, image_embedding_size=(64, 64), input_image_size=(1024, 1024)\n",
    "        )\n",
    "        mask_decoder = MaskDecoder(\n",
    "            num_multimask_outputs=3, transformer_dim=256\n",
    "        )\n",
    "        \n",
    "        # 2. Combine components into the main DetAny3D structure\n",
    "        model = TwoWayTransformer(\n",
    "            image_encoder=image_encoder,\n",
    "            prompt_encoder=prompt_encoder,\n",
    "            mask_decoder=mask_decoder,\n",
    "            # Placeholder for other required parameters\n",
    "        )\n",
    "        print(\"✅ Model architecture instantiated.\")\n",
    "\n",
    "        # 3. Load checkpoint weights\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        state_dict = checkpoint.get('state_dict', checkpoint)\n",
    "            \n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        print(\"✅ Model weights loaded and set to evaluation mode.\")\n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ FATAL MODEL LOAD ERROR: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e9dc6ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"/app/DetAny3D/detect_anything/configs/loss_analysis.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "915bf010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from /app/DetAny3D/checkpoints/dino_ckpts/dinov2_vitl14_pretrain.pth with: _IncompatibleKeys(missing_keys=['register_tokens'], unexpected_keys=[])\n",
      "❌ FATAL MODEL LOAD ERROR: only support depth 32\n",
      "--- Using MOCKMODEL for Inference ---\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "# Build the model using the function\n",
    "detany3d_model = build_detany3d_model(cfg, CHECKPOINT_PATH, DEVICE)\n",
    "\n",
    "# Define the model to be used for inference\n",
    "if detany3d_model:\n",
    "    model = detany3d_model\n",
    "    print(\"--- Using REAL DetAny3D Model for Inference ---\")\n",
    "else:\n",
    "    # Fallback to MockModel if real model failed to load\n",
    "    print(\"--- Using MOCKMODEL for Inference ---\")\n",
    "    class MockModel:\n",
    "        def eval(self): pass\n",
    "        def __call__(self, data):\n",
    "            gt_box_tensor = data['gt_box'][0]\n",
    "            gt_box_np = gt_box_tensor.cpu().numpy()\n",
    "            pred_box_np = gt_box_np.copy()\n",
    "            pred_box_np[6] = limit_period(gt_box_np[6] + np.random.uniform(-0.5, 0.5), period=2 * np.pi)\n",
    "            pred_box_np[0:6] += np.random.randn(6) * 0.1 \n",
    "            pred_box_tensor = torch.from_numpy(pred_box_np).float()\n",
    "            return [[pred_box_tensor]] \n",
    "    model = MockModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e146dab8",
   "metadata": {},
   "outputs": [
    {
     "ename": "BoxTypeError",
     "evalue": "Box expected at most 1 argument, got 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBoxTypeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m box_size \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(ann_record[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     26\u001b[0m box_rotation \u001b[38;5;241m=\u001b[39m Quaternion(ann_record[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrotation\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 28\u001b[0m gt_nusc_box \u001b[38;5;241m=\u001b[39m \u001b[43mBox\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbox_center\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox_rotation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mann_record\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategory_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# 2. Get the transformations (poses)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m ego_pose_record \u001b[38;5;241m=\u001b[39m nusc\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mego_pose\u001b[39m\u001b[38;5;124m'\u001b[39m, sd_record[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mego_pose_token\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/box/box.py:297\u001b[0m, in \u001b[0;36mbox.box.Box.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mBoxTypeError\u001b[0m: Box expected at most 1 argument, got 3"
     ]
    }
   ],
   "source": [
    "# --- DATA SELECTION AND INFERENCE SETUP ---\n",
    "if nusc is None:\n",
    "    raise RuntimeError(\"NuScenes object not initialized. Cannot proceed.\")\n",
    "\n",
    "cam_data_tokens = [sd['token'] for sd in nusc.sample_data if sd['channel'] == CAM_CHANNEL]\n",
    "random_sd_token = random.choice(cam_data_tokens)\n",
    "sd_record = nusc.get('sample_data', random_sd_token)\n",
    "sample_record = nusc.get('sample', sd_record['sample_token'])\n",
    "IMAGE_PATH_NUSCENES = os.path.join(nusc.dataroot, sd_record['filename'])\n",
    "\n",
    "# Get Calibration\n",
    "cs_record = nusc.get('calibrated_sensor', sd_record['calibrated_sensor_token'])\n",
    "P_intrinsic = np.array(cs_record['camera_intrinsic'])\n",
    "CALIB_MATRIX_NUSCENES = np.hstack((P_intrinsic, np.zeros((3, 1))))\n",
    "\n",
    "# Extract GT and Transform (CONCEPTUALLY COMPLETE STEP)\n",
    "ann_tokens = sample_record['anns']\n",
    "if not ann_tokens:\n",
    "    raise ValueError(\"Selected sample has no annotations. Please re-run to select a different sample.\")\n",
    "\n",
    "ann_record = nusc.get('sample_annotation', ann_tokens[0])\n",
    "\n",
    "# 1. Create the nuScenes Box object (FIXED TYPE ERROR)\n",
    "box_center = np.array(ann_record['translation'], dtype=np.float32)\n",
    "box_size = np.array(ann_record['size'], dtype=np.float32)\n",
    "box_rotation = Quaternion(ann_record['rotation'])\n",
    "\n",
    "gt_nusc_box = Box(box_center, box_size, box_rotation, name=ann_record['category_name'])\n",
    "\n",
    "# 2. Get the transformations (poses)\n",
    "ego_pose_record = nusc.get('ego_pose', sd_record['ego_pose_token'])\n",
    "ego_pose_rotation = Quaternion(ego_pose_record['rotation'])\n",
    "cs_translation = cs_record['translation']\n",
    "cs_rotation = Quaternion(cs_record['rotation'])\n",
    "\n",
    "# 3. Transform the GT box: World -> Ego -> Sensor\n",
    "gt_nusc_box.translate(-np.array(ego_pose_record['translation']))\n",
    "gt_nusc_box.rotate(ego_pose_rotation.inverse)\n",
    "gt_nusc_box.translate(-np.array(cs_translation))\n",
    "gt_nusc_box.rotate(cs_rotation.inverse)\n",
    "\n",
    "# 4. Convert to the 7-DoF array [x, y, z, l, w, h, yaw]\n",
    "center = np.array(gt_nusc_box.center)\n",
    "# nuScenes size is [w, l, h]. Standard 3D is often [l, w, h]. Adjusting order.\n",
    "dims = np.array([gt_nusc_box.l, gt_nusc_box.w, gt_nusc_box.h], dtype=np.float32) \n",
    "# Yaw angle from the quaternion (around Y-axis in the camera sensor frame)\n",
    "yaw = gt_nusc_box.orientation.yaw_pitch_roll[0]\n",
    "\n",
    "GT_BOX_7DIM = np.array([center[0], center[1], center[2], dims[0], dims[1], dims[2], yaw], dtype=np.float32)\n",
    "\n",
    "# Create input dictionary\n",
    "input_data = {\n",
    "    'img': torch.randn(1, 3, 900, 1600), \n",
    "    'img_metas': [{'cam_intrinsic': P_intrinsic, 'cam_extrinsic': np.array(cs_record['rotation'])}], \n",
    "    'gt_box': [torch.from_numpy(GT_BOX_7DIM).float()]\n",
    "}\n",
    "\n",
    "# RUN INFERENCE\n",
    "with torch.no_grad():\n",
    "    model_output = model(input_data)\n",
    "\n",
    "# Extract predicted 7-DoF box\n",
    "PRED_BOX_7DIM = model_output[0][0].detach().cpu().numpy()\n",
    "\n",
    "# --- Loss Calculation ---\n",
    "gt_yaw = GT_BOX_7DIM[6]\n",
    "pred_yaw = PRED_BOX_7DIM[6]\n",
    "angle_diff_yaw = limit_period(pred_yaw - gt_yaw, period=2 * np.pi)\n",
    "loss_theta = np.abs(angle_diff_yaw)\n",
    "loss_phi = 0.0 # Placeholder\n",
    "\n",
    "# Assemble data structure for visualization\n",
    "data_point_nusc = {\n",
    "    'index': random_sd_token,\n",
    "    'image_path': IMAGE_PATH_NUSCENES,\n",
    "    'calib': CALIB_MATRIX_NUSCENES,\n",
    "    'gt_box': GT_BOX_7DIM,  \n",
    "    'pred_box': PRED_BOX_7DIM,\n",
    "    'loss_phi': loss_phi,\n",
    "    'loss_theta': loss_theta,\n",
    "}\n",
    "\n",
    "print(f\"\\n✅ Data and Inference Setup Complete for image: {os.path.basename(IMAGE_PATH_NUSCENES)}\")\n",
    "print(f\"  GT Yaw: {np.degrees(gt_yaw):.1f}° | Pred Yaw: {np.degrees(pred_yaw):.1f}°\")\n",
    "print(f\"  Calculated Loss_Theta (Yaw): {loss_theta:.3f} rad ({np.degrees(loss_theta):.1f}°)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdbfeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- VISUALIZATION FUNCTION ---\n",
    "\n",
    "def visualize_nuscenes_loss(data_point):\n",
    "    \n",
    "    try:\n",
    "        # Load the actual image \n",
    "        img = plt.imread(data_point['image_path']) \n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\n❌ ERROR: Image not found at {data_point['image_path']}. Using placeholder.\")\n",
    "        img = np.ones((900, 1600, 3), dtype=np.uint8) * 255\n",
    "        \n",
    "    calib = data_point['calib']\n",
    "    gt_box = data_point['gt_box']\n",
    "    pred_box = data_point['pred_box']\n",
    "    loss_theta = data_point['loss_theta']\n",
    "    \n",
    "    # 1. Hybrid Box (GT Yaw, Pred Center/Dim)\n",
    "    hybrid_box = pred_box.copy()\n",
    "    hybrid_box[6] = gt_box[6] \n",
    "    \n",
    "    # 2. Get 3D Corner Points\n",
    "    gt_corners = get_box_corners(gt_box)\n",
    "    pred_corners = get_box_corners(pred_box)\n",
    "    hybrid_corners = get_box_corners(hybrid_box)\n",
    "    \n",
    "    # 3. Calculate Corner Distortion (L2 distance)\n",
    "    distortion = np.linalg.norm(pred_corners - gt_corners, axis=1)\n",
    "    max_distortion_idx = np.argmax(distortion)\n",
    "    \n",
    "    # --- Plotting ---\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"nuScenes Loss Analysis | Yaw Loss: {loss_theta:.3f} rad ({np.degrees(loss_theta):.1f}°)\")\n",
    "    \n",
    "    # Draw Boxes\n",
    "    project_3d_to_2d(gt_corners, calib, color='g', linestyle='-', label='Ground Truth (GT)')\n",
    "    project_3d_to_2d(pred_corners, calib, color='r', linestyle='-', label='Prediction (Pred)')\n",
    "    project_3d_to_2d(hybrid_corners, calib, color='y', linestyle='--', label='Hybrid (GT Yaw, Pred Center/Dim)')\n",
    "    \n",
    "    # Highlight the most distorted corner\n",
    "    distorted_corner_2d = project_3d_to_2d(pred_corners[max_distortion_idx:max_distortion_idx+1], \n",
    "                                           calib, color=None, linestyle=None, return_coords=True)\n",
    "    \n",
    "    plt.scatter(distorted_corner_2d[0], distorted_corner_2d[1], \n",
    "                s=200, color='magenta', marker='*', label=f'Max Distortion Corner ({distortion[max_distortion_idx]:.2f}m)', zorder=10) \n",
    "    \n",
    "    plt.legend()\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# --- EXECUTION ---\n",
    "# Run the visualization\n",
    "visualize_nuscenes_loss(data_point_nusc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
